<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ChatGPT Demystified</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: "warnock-pro", Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
        }
        h1, h2 {
            color: #2C3E50;
        }
        .navbar {
            margin-bottom: 20px;
        }
    </style>
    <script src="data/wordvecs10000.js"></script>
</head>
<body>

    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">ChatGPT Demystified</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link active" href="#">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/word2vec.html">Embeddings</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/game.html">Game</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/tokenizer2.html">Tokenizer</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <div class="container">
        <header class="text-center mb-4">
            <h1>ChatGPT Demystified</h1>
            <p class="lead">An inside look into how ChatGPT predicts text and powers conversations.</p>
        </header>

        <!-- Section: At a Glance -->
        <section id="at-a-glance" class="mb-5">
            <h2>At a Glance</h2>
            <p>
                You may have heard of ChatGPT, a powerful language model that can generate human-like text. But how does it really work? Here's a quick overview:
            </p>
            <ul>
                <li>ChatGPT learned by reading tons of text from books, websites, and other sources to understand how people use language. It doesn’t “know” things in the way people do, but it has seen many examples of language and ideas.</li>
                <li>When you give ChatGPT a prompt, it uses patterns it learned from the data to predict the most likely next word in a sequence. This is how it generates text that seems coherent and contextually relevant.</li>
                <li>As it predicts new words, those words get added to the context, and a new word is generated based on that </li>
            </ul>
            <p>
                <!-- ChatGPT is a type of language model that generates text by predicting the most likely next word in a sequence, based on patterns it has learned from vast amounts of data. 
                Essentially, ChatGPT takes in a prompt, analyzes its context, and determines the next word through probabilistic reasoning.
                This approach allows it to produce responses that seem coherent and contextually relevant, making it effective in generating human-like conversations and answers. -->
            </p>
        </section>
        <section id="how-it-works">
            <h2>How ChatGPT Sees your problem</h2>
            <p>
                Lets play a game to understand how ChatGPT works. Each round a word of the true sentence will be revealed, and both you and a language model will attempt to predict the next word.  
            </p>
            <!--Add a note that this is not chatGPT, but the less powerfull llama model-->
            
            <iframe src="game.html" width="100%" height="500px" frameborder="0"></iframe>
            <p>
                <strong>Note:</strong> This is a simplified version of how ChatGPT works, using a less powerful model called Llama. The goal is to give you a hands-on experience of predicting words based on context.
            </p>
        </section>
        <section id="how-it-works">
            <h2>What is a model?</h2>
            <p>
                Lets imagine you want to predict someones salary in a certain role.
                You could try to guess it based on your own intuition, but how do you know if your guess is justified?
                <br/>
                A mathematical model is a representation of a system, process, or concept using mathematical concepts and language. 
                <br/>
                Lets make a very simple model to predict someones salary based on their years of experience. The equation will be:
                <br/>
                <code>salary = slope * experience + intercept</code>
                <br/>
                Where "slope" and "intercept" are numbers that we can adjust to make the model fit the data better. In this instance, the intercept represents the starting salary, and the slope represents how much the salary increases with each year of experience.
            </p>
            <h3>"Training" a model</h3>
            <p>
                If I told you that someone with 2 years of experience makes $18/hour, and 2 people with 5 years of experience make $25 and $30/hour, could you use that information to make a better guess?
                This is the idea behind training. You use the relevant information you have to make a better guess.
                <br/>
                Lets see how this works in practice. Try adjusting the "slope" and "intercept" sliders to fit your model to the data.
            </p>
            <iframe id="modeliframe" src="models.html" width="100%" height="700px" frameborder="0"></iframe>
            <p>
                But wait, how can a computer do this? Most people can eyeball a line and see how well it fits the data, but a computer needs a more systematic approach. 
                <br/>
                First we need to define what "better" means. In this case, we can say that a better model is one that makes predictions that are closer to the true values.
                <br/>
                Specifically, we can define the "residual" as the difference between the true value and the value the model predicts. Click the button below to see the residuals for the current model.
                <br/>
                <br/>
                <button class="btn btn-primary" id="showRisiduals">Show Residuals</button>
                <p>
                    Now, how can we adjust the slope and intercept to make the residuals closer to 0? This is where "gradient descent" comes in.
                    <br/>                    
                </p>
                <p>
                    Imagine you where to make a very small adjustment to the slope and intercept. Would the residuals get smaller or larger? If they get smaller, you should keep adjusting in that direction. If they get larger, you should adjust in the opposite direction.
                    <br/>
                    This is the basic idea behind gradient descent. You start with a random guess for the slope and intercept, and then adjust them in the direction that makes the residuals smaller.
                    <br/>
                    <!--Accordion with information on backpropegation-->
                <div class="accordion" id="backpropAccordion">
                    <div class="accordion-item">
                      <h2 class="accordion-header" id="headingOne">
                        <button class="accordion-button collapsed bg-info" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                          <!-- Question mark icon -->
                            <div class="d-inline-flex align-items-center ml-5">
                            </div>
                            More about Model Training
                        </button>
                        </h2>
                        <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne" data-bs-parent="#backpropAccordion">
                            <div class="accordion-body">
                                <p>
                                    Gradient descent is a very simple but powerful algorithm that is used to optimize the parameters of a model.
                                    <br/>
                                    However, it has a number of limitations on its own and has been extended in many ways to make it more powerful.
                                    <br/>
                                    <h5>Limitation 1: Gradient Calculation</h5>
                                    One of the key challenges is getting this "gradient" (the direction we should adjust the parameters). In our simple model we can "make tiny adjustments" and re-run it but in more complex models this is not feasible.
                                    <br/>
                                    This is where "backpropagation" comes in. Backpropagation is a method used to calculate the gradient of very complicated, multi-layered models like ChatGPT.
                                    <br/>
                                    The basic idea is to calculate the gradient of the output of the model with respect to each parameter in the model. This is done by using the chain rule from calculus to calculate the gradient of each layer with respect to the layer before it.
                                    <br/>
                                    This is a very difficult concept to understand, but it is the key to training models like ChatGPT.
                                    <br/>
                                    Here are some resources to learn more about backpropagation:
                                    <ul>
                                        <li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">3Blue1Brown Video</a></li>
                                        <li><a href="https://xnought.github.io/backprop-explainer/">Visual Explanation</a></li>
                                    </ul>
                                    <h5>Limitation 2: Local Minima</h5>
                                    Another key limitation of gradient descent (and residual-minimizing training in general) is that it can get stuck in local minima. This is when the model finds a set of parameters that are better than the ones immediately next to it, but not as good as the best possible set of parameters.
                                    <br/>
                                    You can imagine local minima as a valley in a mountain range. If you start at the bottom of a valley, every direction you walk will take you up the mountain. But if you want to get to the lowest possible point on earth, you will have to climb out of the valley and find a new one.
                                    <br/>
                                    This is usually accomplished by more complicated optimization algorithms like "Adam" or "RMSProp".
                                    <br/>
                                    <h5>Limitation 3: Overfitting</h5>
                                    A final major limitation of gradient descent is that it can lead to "overfitting". This is when the model learns the training data too well, and is unable to generalize to new data.
                                    <br/>
                                    You can think of this as a student who memorizes the answers to a test, but doesn't understand the underlying concepts. They will do well on that test, but will struggle with new questions.
                                    <br/>
                                    This is usually addressed by using more data, or by using "regularization" techniques that penalize the model for being too complex.
                                    <br/>
                                    The reason penalizing complexity works is the model doesn't have enough "room" to memorize the training data, and is forced to learn the underlying patterns.
                                    <br/>
                                    For example, in our salary model, if we had added a bunch of extra parameters (like the number of letters in the persons name, the color of their shoes etc) the model could have made a perfect prediction on our initial data by forming a hyperplane that connects every point, and has perfect accuracy.
                                    <br/>
                                    Mathematically, this is done by adding a term to the loss function that penalizes the model for having large weights. This makes the model prefer simpler explanations that generalize better.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                    Lets see how this works in practice. Click the button below to start the gradient descent algorithm.
                </p>
                <button class="btn btn-primary" id="startGradientDescent">Start Gradient Descent</button>
                <p>
                    Alright! now we have a trained model! 
                    Lets say we used the people in our team to train the model, 
                    and now we want to see how well it does on a random team within our company. This is called "testing" or "evaluating" the model.
                    This is an especially important step because <b>models with predictive power</b> are 
                </p>
                <p>
                    Click the button below to see how well our model does on an unseen dataset with similar characteristics.
                </p>

                <button class="btn btn-primary" id="testModel">Test Model</button>
                <p>
                    Pretty good! It seems like our model is able to predict the salary of people with a similar background to the people we used to train the model.
                    <br/>
                    This is the basic idea behind machine learning. You use data to train a model, and then test it on new data to see how well it generalizes.
                    <br/>
                    ChatGPT is vastly more complicated, but it operates on the same principle. 
                    In its case, however, instead of 2 sliders it has over <b>1.5 billion parameters</b> that it adjusts to make the best predictions it can.
                    As you can imagine, training a model like ChatGPT is a massive undertaking that requires a lot of data and computational power.

                </p>
            </p>
        </section>
        <!-- Additional Content (Placeholder for future sections) -->
        <section id="how-it-works">
            <h2>Under the Hood</h2>
            <p>
                OK, so how does ChatGPT actually predict the next word? Whats the intuition behind what these billions of parameters mean?
                Here's a simplified version of ChatGPT's inner workings:
            </p>
            <ol>
                <li>First, the text is <b>broken into tokens</b>. These are usually words, but can be fragments of words</li>
                <li>Next, the model tries to <b>represent each token with an embedding</b> (a list of numbers that encodes the word's meaning).</li>
                <li>Then, the model repeatedly <b>updates the meaning</b> of each token based on the surrounding context</li>
                <li>Each time it updates the meaning, it will also <b>bake world knowledge</b> into the meaning</li>
                <li>Finally, the model <b>predicts the next word</b> based on the meaning of the text so far</li>
            </ol>
            <p>
                Understanding each of these steps in more detail can help you understand how ChatGPT works and what its strengths and limitations are.
            </p>
            <h3>Step 1: Tokenize the text</h3>
            <p>
                Tokenization is the process of breaking text into smaller pieces called tokens. These tokens can be words, subwords, or characters, depending on the model and the task.
                <br/>
                Lets see how this works in practice. Enter some text in the box below and click the "Tokenize" button to see how the text is broken into tokens. You can then click the "Convert to Token IDs" button to see how the tokens are represented as numbers to be fed into the model.
            </p>
            <iframe id="tokeniframe" src="tokenizer2.html" width="100%" height="500px" frameborder="0"></iframe>
            <!-- Question Accordion: -->
            <h3>Step 2: Embed the tokens</h3>
            <p>
                Now we have a bit of a problem. Since our language model is a mathematical function we need some way to represent these words as numbers.
                More difficult still, we need to represent them in a way that captures their meaning so that the model can actually learn from them.
                <br/>
                This is where "word embeddings" come in. Word embeddings are a way to represent words as a list of numbers in a way that captures their meaning.
                These numbers are not assigned, but learned by the model as it reads through millions of examples of text. 
                <br/>
                Lets start with looking at what a word embeddings for the word "cat" looks like:
                <div id="catvec">

                </div>
                <script>
                    const catvec = wordVecs['cat'];
                    const catvechtml = catvec.map((v) => `<span>${v.toFixed(2)}</span>`).join(', ');
                    document.getElementById('catvec').innerHTML = `<code style="font-size:10px">[${catvechtml}]</code>`;
                </script>
                <br/>
                Ohh no. That's a lot of numbers. How can we make sense of this?
                <br/>
                Well maybe a good place to start is to see which words have similar embeddings. Lets see which words have embeddings that are close to the word "cat".
                <br/>
            </p>
            <iframe src="word2vec.html?noWordAlgebra=1" width="100%" height="700px" frameborder="0"></iframe>
            <p>
                But wait there's more! The embeddings represent more than distances between words. 
                They can also do some simple analogies if we think of them mathematically. 

                For example, what does "brother" - "man" + "woman" equal? Lets see if this is encoded in these embeddings!
            </p>
            <iframe src="word2vec.html?noSimilarWords=1" width="100%" height="700px" frameborder="0"></iframe>
            <p>
                Pretty cool right? These embeddings can capture some of our intuitions about language. 
                <br/>
                More precisely, <b>directions in these embeddings encode properties of the words</b>. 
                This is why "brother" - "sister" is very similar to "man" - "woman". They are both encoding the concept of gender.
            </p>
            <!-- collapsed bs4 Accordian below with more information about embeddings-->
            <div class="accordion" id="embeddingAccordion">
                <div class="accordion-item">
                  <h2 class="accordion-header" id="headingOne">
                    <button class="accordion-button collapsed bg-info" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                      <!-- Question mark icon -->
                        <div class="d-inline-flex align-items-center ml-5">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-question-circle" viewBox="0 0 16 16">
                                <path d="M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14m0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16"/>
                                <path d="M5.255 5.786a.237.237 0 0 0 .241.247h.825c.138 0 .248-.113.266-.25.09-.656.54-1.134 1.342-1.134.686 0 1.314.343 1.314 1.168 0 .635-.374.927-.965 1.371-.673.489-1.206 1.06-1.168 1.987l.003.217a.25.25 0 0 0 .25.246h.811a.25.25 0 0 0 .25-.25v-.105c0-.718.273-.927 1.01-1.486.609-.463 1.244-.977 1.244-2.056 0-1.511-1.276-2.241-2.673-2.241-1.267 0-2.655.59-2.75 2.286m1.557 5.763c0 .533.425.927 1.01.927.609 0 1.028-.394 1.028-.927 0-.552-.42-.94-1.029-.94-.584 0-1.009.388-1.009.94"/>
                              </svg>
                        </div>
                      More about embeddings
                    </button>
                  </h2>
                  <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne" data-bs-parent="#embeddingAccordion">
                    <div class="accordion-body">
                      <p>
                        Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. 
                        They are learned from data and are able to capture semantic relationships between words. 
                        <br/>
                        There are many ways to learn word embeddings, but one common method is to use a neural network to predict the context of a word based on its surrounding words. 
                        The weights of the neural network are then used as the word embeddings.
                        <br/>
                        These word embeddings are a smaller version of the <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a> embeddings, which are trained on a large corpus of text to capture the meaning of words.
                        <br/>
                        <i>
                            Note: GPT does not use Word2Vec embeddings directly, Word2Vec is just a common example of word embeddings with relatively few dimensions. GPT uses embeddings with 768 dimensions, making them more difficult to work with.
                            <br/>
                            Both of the demonstrated phenomena (Similarity and analogies) have been demonstrated for GPT embeddings as well.
                        </i>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
              <br/>
              <h3 >Step 3: Update the token meanings</h3>
              <p>
                Word embeddings are pretty neat, but you may have noticed a problem. The same word can have dramatically different meanings depending on the context.
                <br/>
                For example, "bat" can refer to a flying mammal, a piece of sports equipment, or a verb meaning to hit something.
                <br/>
                This is by far the most difficult part of the language model. In fact, this method of updating the meaning of words based on context is the main innovation that paved the way for chatGPT.
                <br/>
                To do this, the model runs the embeddings through a series of "layers" that look at all the embeddings and update them all together. 
                <br/>
                You can think of this like the model assigning an initial meaning to each word, then editing each meaning to make the text make more sense as a whole.
                <br/>
              </p>
              <h4>A Motivating Example</h4>
              <p>
                Lets think about the word "bat" again. Lets see how the model embeds the word "bat" in a variety of contexts, and see if we can separate the meanings.
                <br/>
                To do this, we will look at a special projection of the embeddings called "t-SNE". The exact workings are not important, but the key idea is it tries to keep distances and structure between the embeddings on a 2 dimensional graph similar to the original distances
              </p>
              <iframe src="tsne_visualization.html" width="100%" height="500px" frameborder="0"></iframe>
              <p>
                As you can see, the model is able to separate the different meanings of the word "bat" into different regions of the graph. But how does it do this?
                <br/>
                Its easiest to see this by looking through each layer of the model and seeing how the embeddings change. Click the "Next Layer" button below to see how the embeddings change as they pass through the model.
              </p>
            <iframe src="layer_visualization.html" width="100%" height="700px" frameborder="0"></iframe>
            <p>
                As you can see, the embeddings change dramatically as they pass through the model, and capture more and more information about the context of the word.
                <br/>
                Technically, this is accomplished through a process called "attention". You can think of this as the model adjusting the meaning of a word by first deciding which other words are most important, then updating the meaning based on those words.
            </p>
            <!-- Info accordion with more information on attention-->
            <div class="accordion" id="attentionAccordion">
                <div class="accordion-item">
                  <h2 class="accordion-header" id="headingTwo">
                    <button class="accordion-button collapsed bg-info" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                      <!-- Question mark icon -->
                        <div class="d-inline-flex align-items-center ml-5">
                        </div>
                        But what is attention&nbsp;<i>really</i>&nbsp;doing?
                    </button>
                    </h2>
                    <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo" data-bs-parent="#attentionAccordion">
                        <div class="accordion-body">
                            <p>
                                Attention is a mechanism that allows a model to focus on certain parts of the input when making predictions.
                                <br/>
                                In the case of language models like GPT, attention is used to decide which words are most important when updating the meaning of a word.
                                <br/>
                                In the below example, you can see what the model is "paying attention to" when updating the meaning of the words in a sentence.

                            </p>
                            <!-- assets/img/attention_heatmap.png correctly sized and centered with caption-->
                            <figure class="text-center">
                                <img src="assets/img/attention_heatmap.png" style="width:70%;" class="img-fluid" alt="Attention Heatmap">
                                <figcaption class="text-muted">An example of an attention heatmap in a language model. 
                                    Keep in mind this is only the first layer, and the "heads" (parts of the model that pay attention to different parts of the input) have been averaged </figcaption>
                            </figure>
                            <p>
                                From this heatmap, you can see that the model updates the meaning of "bat" based on the word "baseball" (row 3 col 2) more than the word "baseball" based on the word "bat" (row 2 col 3).
                                <br/>
                                This aligns with our intuition, that we learn more about the meaning of "bat" from the word "baseball" than the other way around.
                                <br/>
                                <i>
                                    Note: [CLS] and [SEP] are special tokens that the model uses to mark the beginning and end of a sentence. Interpreting their attention values is more difficult.
                                </i>
                                <br/>
                                <br/>
                                Looking at a single head mathematically, It consists of "query" vectors, "key" vectors, and "value" vectors. 
                                The steps are as follows, for each query / key pair:
                                <ol>
                                    <li>Calculate the Query vector for each word by multiplying the word embedding by the query vectors. You can think of it like the word "asking questions" about its context </li>
                                    <li>Calculate the Key vector for each word by multiplying the word embedding by the key vectors. You can think of it like the word "answering questions" about its context </li>
                                </ol>
                                After this, the next steps for each word are:
                                <ol>
                                    <li>Calculate the attention score for each word by taking the dot product of the query and key vectors. This tells us how much the word "cares" about the other word (how much it answers its questions)</li>
                                    <li>Normalize the attention scores by applying a softmax function. This makes the scores add up to 1, so they can be used as weights</li>
                                </ol>
                                <br/>
                                This can be very difficult to visualize, as neither the query nor key vectors are directly interpretable. But the idea is the model uses these vectors to decide which words are most important when updating the meaning of a word.

                                In practice, it helps to have multiple "heads" that can capture different patterns in the data. At the end of each layer, the model combines the information from all the heads to update the meaning of each word.
                                Below you can see what each head is paying attention to when updating the meaning of the word "bats" in the sentence "Baseball bats are about three feet long.".
                            </p>
                            <figure class="text-center">
                                <img src="assets/img/attention_heads_matrix.png" style="width:70%;" class="img-fluid" alt="Attention Heatmap">
                                <figcaption class="text-muted">
                                    An example of what each "head" in the model is paying attention to when updating the meaning of "bats". Each row represents a different head, and each column represents a different word in the input.
                                </figcaption>
                            </figure>
                            <p>
                                For more information on attention, check out the following resources:
                                <ul>
                                    <li>This incredible <a href="https://www.youtube.com/watch?v=eMlx5fFNoYc">3B1B Video</a></li>
                                    <li>Original Transformer architecture paper: <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
                                    <li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">The Illustrated Transformer</a></li>
                                </ul>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br/>
            <h3>Step 4: Bake in world knowledge</h3>
            <p>
                Okay, so the model can update the meaning of words based on context. 
                But there is much more to language than just understanding sentences on their own. 
                <br/>
                For example, if I say "Michael Jordan is the greatest player of all time" you probably know I'm talking about basketball, even if I never mentioned the sport.
                <br/>
                Some of this knowledge is already present in the embeddings, such as capitol - country relationships.
                <br/>
                But there is way too much information to cram into even very large embeddings. 
                This is where the models "MLP" (Multi Layer Perceptron) layer comes in.
                <br/>
            </p>
            <h4>Steps of adding this information</h4>
            <p>
                The model is really doing 2 things in this layer. The first is asking a series of questions about the current embeddings, and the second is adding that information into the embeddings.
                <br/>
                These questions are <i>almost</i> yes / no. The primary reason they are not is to allow the model to train more effectively by making small adjustments.
                <br/>
                Some of examples of questions the model might "ask the embeddings" in the first layer are:
                <ul>
                    <li>Is this Micheal Jordan?</li>
                    <li>Is this the Soviet Union in 1970?</li>
                    <li>Is this "The Godfather"</li>
                    <li>Is this a bat that flies?</li>
                </ul>                
                <b>Note that these are questions that can be answered with only the sentence itself</b>.
                <br/>
                The model then uses its "knowledge" to construct new embeddings, that add the additional world information to the embeddings.
                <br/>


                <!-- To both bake in information and update the embeddings accordingly this layer starts with "asking questions".
                <br/>
                More exactly, this is is done by looking for certain patterns in the embeddings like "Michael Jordan". at this point, the embeddings capture their relation but <b>not the meaning of the relation</b>.
                <br/>
                This is like if you told someone from 1900 about "Michael Jordan". They would know that this is a person, but not anything else.
                <br/>
                The model "knows" in this layer, that certain relationships have extra information.
                <br/>
                Mathematically, "questions" are "asked" of the embeddings by seeing how similar the embedding is to an embedding that represents the relationship in question. 
                <br/>
                Interpreting these questions is almost impossible, since they are learned from data and do not nessisarily have an easy interpretation like "is this Michael Jordan?". 
                <br/> -->
            </p>
            <!-- Accordian labeled "How do you "ask a question" mathematically?"-->
            <div class="accordion" id="questionAccordion">
                <div class="accordion-item">
                  <h2 class="accordion-header" id="headingThree">
                    <button class="accordion-button collapsed bg-info" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                      <!-- Question mark icon -->
                        <div class="d-inline-flex align-items-center ml-5">
                        </div>
                        How do you "ask a question" mathematically?
                    </button>
                    </h2>
                    <div id="collapseThree" class="accordion-collapse collapse" aria-labelledby="headingThree" data-bs-parent="#questionAccordion">
                        <div class="accordion-body">
                            <p>
                                One of the the foundations of neural networks is "matrix multiplication". This is where all the processing actually happens. 
                                <br/>
                                Since the embeddings are a list of numbers (known as a vector), these matrix multiplications can be thought of as a series of "dot products" between the rows of the matrix and the embedding.
                                <br/>
                                A dot product is a way to multiply two vectors together to get a single number. This number represents how similar the two vectors are.
                                <br/>
                                So to "ask a question" of the embeddings, the model multiplies the embeddings by a matrix that represents the question.
                                <br/>
                                The result is a list of numbers that represent how similar the embeddings are to the question. This is then used to update the embeddings.
                                <br/>
                                After these dot products are calculated, they are passed through a function which throws away negative values. This is called a "ReLU" function, and is used to convert these similarities into almost yes / no answers.
                                <br/>
                                <br/>
                                <h5>A Simplified Example</h5>
                                In 2 dimensions, you could imagine the embedding as a line pointing from the center in a certain direction. 
                                <br/>
                                You can then imagine a question as a line pointing in a different direction. The dot product of these two lines will be greater the more similar the two lines are.
                                <br/>
                                See the visualization below to play around with this idea.
                            </p>
                            <iframe src="dot_product.html" width="100%" height="500px" frameborder="0"></iframe>
                        </div>
                    </div>
                </div>
            <p>
                As the model goes through more layers, it can ask more complex questions and work with information integrated from previous layers.
                <br/>
                The actual questions asked by the model are very difficult to interpret directly, but we can create our own questions and see if the model can answer them.
                <br/>
                This is easiest if we just focus on names, since the correct interpretation of them is almost entirely dependant on prior knowledge.
                <br/>

            </p>

            <h3>Step 5: Predict the next word</h3>
            <p>
                Finally, the model uses the updated meanings of the tokens to predict the next word in the sequence. 
                This is done by running the embeddings through a final layer that converts them into probabilities over the vocabulary.
                <br/>
                The model then selects the word with the highest probability as the next word in the sequence.
                <br/>
                Lets see how this works in practice. Enter a sentence in the box below and click the "Predict Next Word" button to see what the model predicts.
            </p>
            <iframe src="predictor.html" width="100%" height="500px" frameborder="0"></iframe>
            <h3>Putting it All Together</h3>
            <p>
                Throughout this explanation, we have glazed over many of the exact details of how the model works. 
                <br/>
                However, If you understood everything up to this point, I believe you can understand the basic idea of how ChatGPT works.
                <br/>
                Below is a complete interactive diagram of the model, where you can see exactly what happens at each layer.
            </p>
            <iframe src="model.html" width="100%" height="700px" frameborder="0"></iframe>
        </section>
        <section>
            <h2>Conclusion: What Have Language Models Learned?</h2>
            <p>
                Under Construction
            </p>
        </section>
    </div>
    <script>
        var pushOrigin = '*'
        // if we are on centipede5.github.io updata
        if (window.location.hostname == 'centipede5.github.io') {
            pushOrigin = 'https://centipede5.github.io'
        }
        const showRisidualsButton = document.getElementById('showRisiduals');
        showRisidualsButton.addEventListener('click', function(e){
            // check button
            e.target.classList.toggle('btn-primary');
            e.target.classList.toggle('btn-secondary');
            e.target.innerHTML = e.target.innerHTML == 'Show Residuals' ? 'Hide Residuals' : 'Show Residuals';
    
            // signal iframe
            const iframe = document.getElementById('modeliframe');
            const message = {
                type: 'show_loss',
            }
            iframe.contentWindow.postMessage(message, pushOrigin);
            //scroll to modeliframe
            iframe.scrollIntoView();

        });
        const startGradientDescentButton = document.getElementById('startGradientDescent');
        startGradientDescentButton.addEventListener('click', function(e){
            // signal iframe
            const iframe = document.getElementById('modeliframe');
            const message = {
                type: 'start_gradient_descent',
            }
            iframe.contentWindow.postMessage(message, pushOrigin);
            //scroll to modeliframe
            iframe.scrollIntoView();
        });
        const testModelButton = document.getElementById('testModel');
        testModelButton.addEventListener('click', function(e){
            // signal iframe
            const iframe = document.getElementById('modeliframe');
            const message = {
                type: 'generate_data',
            }
            iframe.contentWindow.postMessage(message, pushOrigin);
            //scroll to modeliframe
            iframe.scrollIntoView();
        });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
